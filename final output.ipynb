{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Model to toxic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening a file which contains the list of volgorirty words\n",
    "with open(\"list.txt\") as f:\n",
    "    lines=f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_word=lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "WORDS = Counter(words(open('list1.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCheckToxic(str):\n",
    "    str_new=[]\n",
    "    final_str=[]\n",
    "    words=nltk.word_tokenize(str)\n",
    "    for i in words:\n",
    "        corrected_word=correction(i)\n",
    "        a=i\n",
    "        b=corrected_word\n",
    "        seq = difflib.SequenceMatcher(None,a,b)\n",
    "        d = seq.ratio()*100\n",
    "#     print(d) \n",
    "        if(d>80):\n",
    "            str_new.append(corrected_word)\n",
    "        else:\n",
    "            str_new.append(i)\n",
    "#     print(str_new)\n",
    "    str_new=' '.join(str_new)\n",
    "    final_str.append(str_new)\n",
    "    return final_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS2 = words(open('list1.txt').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "def spell_check(text):\n",
    "    f=[]\n",
    "    li=nltk.word_tokenize(text)\n",
    "    for k in li:\n",
    "        toxic_word = k\n",
    "        j=0\n",
    "        def words(text): return re.findall(r'\\w+', text.lower())\n",
    "        count = 0\n",
    "\n",
    "        \n",
    "        for i in WORDS2:\n",
    "            if i == toxic_word:\n",
    "                count = 2\n",
    "        l=spellCheckToxic(toxic_word)\n",
    "        tc = 0\n",
    "        for i in WORDS2:\n",
    "            if i == l[0]:\n",
    "                tc=1\n",
    "                count = count + 1\n",
    "                break\n",
    "        toxic_word2 = spell(toxic_word)\n",
    "        nc = 0\n",
    "        for i in WORDS2:\n",
    "            if i == toxic_word2:\n",
    "                nc=1\n",
    "                count += 3\n",
    "                break\n",
    "        if count >= 3:\n",
    "            j =1\n",
    "        else:\n",
    "             if(toxic_word == toxic_word2 or l[0] == toxic_word):\n",
    "                j=1\n",
    "        \n",
    "             else:\n",
    "                 j=0\n",
    "        if(j==1):\n",
    "            m=spell(k)\n",
    "            f.append(m)\n",
    "        else:\n",
    "            m=spellCheckToxic(k)\n",
    "            f.append(m[0])\n",
    "    final_text=\" \".join(f)\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Predicting toxic with Bag_of_word (Rule based Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning_data(line):\n",
    "    line=spell_check(line)\n",
    "    line=re.sub(r\"(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\",\"\",line)\n",
    "        # remove the unnecessary punctuations\n",
    "    line=re.sub(r\"[^a-zA-Z ]\",\"\",line)\n",
    "    line=re.sub(r'(\\w)\\1{2,}',r'\\1',line)\n",
    "\n",
    "        # remove number\n",
    "    line=re.sub(r\"[0-9]\",\"\",line)\n",
    "    \n",
    "    tokens_sent=word_tokenize(line)\n",
    "    \n",
    "        \n",
    "    return tokens_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 1 Bag of words\n",
    "def Bag_of_Words(data):\n",
    "    line=set(Cleaning_data(data))\n",
    "    \n",
    "    toxic=False\n",
    "    count=0\n",
    "    for word in line:\n",
    "        if word in toxic_word:\n",
    "            count+=1\n",
    "    if(count>0):\n",
    "        toxic=True\n",
    "    return toxic\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n"
     ]
    }
   ],
   "source": [
    "k=Bag_of_Words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning Model for Toxic comment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading  the datasets \n",
    "train_set=pd.read_csv(\"train.csv\")\n",
    "test_set=pd.read_csv(\"test.csv\")\n",
    "test_set_labels=pd.read_csv(\"test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the all categorical columsn in to one colume i.e toxic,serve,insult,obsecen,threat=== toxic\n",
    "index=train_set.loc[(train_set.toxic==1)|(train_set.severe_toxic==1)|(train_set.obscene==1)|(train_set.threat==1)|(train_set.insult==1)|(train_set.identity_hate==1)].index\n",
    "new_column=list(train_set.toxic)\n",
    "for i in index:\n",
    "    new_column[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the alredy Preprocessed Data which was steeming and removed the stopword\n",
    "preprocessed_data=pd.read_csv(\"processed.csv\")\n",
    "preprocessed_data[\"toxic\"]=new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww ! match background colour 'm seem stuck ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man , 'm realli tri edit war . 's guy cons...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>`` ca n't make real suggest improv - wonder se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, sir , hero . chanc rememb page 's ?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  explan edit made usernam hardcor metallica fan...      0\n",
       "1  d'aww ! match background colour 'm seem stuck ...      0\n",
       "2  hey man , 'm realli tri edit war . 's guy cons...      0\n",
       "3  `` ca n't make real suggest improv - wonder se...      0\n",
       "4              , sir , hero . chanc rememb page 's ?      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying the data set which is Preprocessed \n",
    "display(preprocessed_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the HTML tags and unnecssary punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning the data \n",
    "## removing the html tags and the some unimportant punctuations\n",
    "def CCleaning_data(line):\n",
    "    line=re.sub(r\"(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\",\"\",line)\n",
    "        # remove the unnecessary punctuations\n",
    "    line=re.sub(r\"[^a-zA-Z ]\",\"\",line)\n",
    "\n",
    "        # remove numbers\n",
    "    line=re.sub(r\"[0-9]\",\"\",line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=[]\n",
    "for sent in preprocessed_data[\"comment_text\"]:\n",
    "    clean_data.append(CCleaning_data(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repacing the comment_text with the clean data\n",
    "preprocessed_data[\"comment_text\"]=clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww  match background colour m seem stuck  th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man  m realli tri edit war  s guy constant...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca nt make real suggest improv  wonder sectio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir  hero  chanc rememb page s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  explan edit made usernam hardcor metallica fan...      0\n",
       "1  daww  match background colour m seem stuck  th...      0\n",
       "2  hey man  m realli tri edit war  s guy constant...      0\n",
       "3   ca nt make real suggest improv  wonder sectio...      0\n",
       "4                    sir  hero  chanc rememb page s       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dispalying the preprocessed dataa\n",
    "display(preprocessed_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data in to the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(preprocessed_data[\"comment_text\"],preprocessed_data[\"toxic\"],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the training_set : (127656,) (127656,)\n",
      "the size of the testing_set  : (31915,) (31915,)\n"
     ]
    }
   ],
   "source": [
    "##  size of the training and testing dataset\n",
    "print(\"the size of the training_set :\",X_train.shape,Y_train.shape)\n",
    "print(\"the size of the testing_set  :\",X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[\"toxic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the document corpus to the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the TF_ID vectorization and for anlayzer is word\n",
    "def convert_text_to_vector():\n",
    "    word_vec=TfidfVectorizer(sublinear_tf=True,analyzer='word',stop_words='english',lowercase=True,token_pattern=r'\\w{3,}',strip_accents='unicode',ngram_range=(1,2),max_features=30000)\n",
    "    char_vec=TfidfVectorizer(analyzer='char',max_features=1000,ngram_range=(2,4),sublinear_tf=True,stop_words='english',strip_accents='unicode')\n",
    "    return word_vec,char_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vctorizer,char_vectorizer=convert_text_to_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting the preprocessed data in to the vectorizer\n",
    "training_features=word_vctorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features=word_vctorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Machine learning Models\n",
    "\n",
    "- Features data : {training_features, testing_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.4, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "Log_classifer=LogisticRegression(C=1.4,solver='sag',max_iter=500)\n",
    "#fit the data\n",
    "Log_classifer.fit(training_features,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting thelabels for the training data\n",
    "log_train_pre=Log_classifer.predict(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the labels for the testing data\n",
    "log_test_pre=Log_classifer.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the accuracy for both train and testing data comapre for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score,matthews_corrcoef,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------F1_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7948511665325825\n",
      "Testing accuracy is :------> 0.7608734885399747\n",
      "\n",
      "-------------MATTHEWS_CORRCOEF_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7877471323673234\n",
      "Testing accuracy is :------> 0.7527513717655351\n",
      "\n",
      "--------------ROC_AUC_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.8405588745004278\n",
      "Testing accuracy is :------> 0.8207041504668859\n",
      "\n",
      "--------------ACCURACY_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.9640439932318104\n",
      "Testing accuracy is :------> 0.958483471721761\n"
     ]
    }
   ],
   "source": [
    "# f1_score accuracy\n",
    "print(\"--------------F1_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",f1_score(Y_train,log_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",f1_score(Y_test,log_test_pre))\n",
    "print()\n",
    "# matthews_corrcoef\n",
    "print(\"-------------MATTHEWS_CORRCOEF_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",matthews_corrcoef(Y_train,log_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",matthews_corrcoef(Y_test,log_test_pre))\n",
    "print()\n",
    "#roc_auc_score\n",
    "print(\"--------------ROC_AUC_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",roc_auc_score(Y_train,log_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",roc_auc_score(Y_test,log_test_pre))\n",
    "print()\n",
    "#accuracy score\n",
    "print(\"--------------ACCURACY_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",accuracy_score(Y_train,log_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",accuracy_score(Y_test,log_test_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)RIDGE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge_classifier=RidgeClassifier(alpha=2,solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=2, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, random_state=None, solver='sag',\n",
       "        tol=0.001)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ridge_classifier.fit(training_features,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on the train set \n",
    "ridge_train_pre=Ridge_classifier.predict(training_features)\n",
    "# Prediction on the test set\n",
    "ridge_test_pre=Ridge_classifier.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------F1_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7705895953757227\n",
      "Testing accuracy is :------> 0.713063320022818\n",
      "\n",
      "-------------MATTHEWS_CORRCOEF_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7686302063045947\n",
      "Testing accuracy is :------> 0.7130052347867653\n",
      "\n",
      "--------------ROC_AUC_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.8197945828313818\n",
      "Testing accuracy is :------> 0.7857679098021939\n",
      "\n",
      "--------------ACCURACY_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.9611377451901987\n",
      "Testing accuracy is :------> 0.9527181576061413\n"
     ]
    }
   ],
   "source": [
    "# f1_score accuracy\n",
    "print(\"--------------F1_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",f1_score(Y_train,ridge_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",f1_score(Y_test,ridge_test_pre))\n",
    "print()\n",
    "# matthews_corrcoef\n",
    "print(\"-------------MATTHEWS_CORRCOEF_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",matthews_corrcoef(Y_train,ridge_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",matthews_corrcoef(Y_test,ridge_test_pre))\n",
    "print()\n",
    "#roc_auc_score\n",
    "print(\"--------------ROC_AUC_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",roc_auc_score(Y_train,ridge_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",roc_auc_score(Y_test,ridge_test_pre))\n",
    "print()\n",
    "#accuracy score\n",
    "print(\"--------------ACCURACY_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",accuracy_score(Y_train,ridge_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",accuracy_score(Y_test,ridge_test_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE BAYES MULTINOMIAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_classifer=MultinomialNB(alpha=0.1)\n",
    "naive_classifer.fit(training_features,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the training and testing features\n",
    "naive_train_pre=naive_classifer.predict(training_features)\n",
    "naive_test_pre=naive_classifer.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------F1_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7682231331313492\n",
      "Testing accuracy is :------> 0.7087994034302758\n",
      "\n",
      "-------------MATTHEWS_CORRCOEF_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7575537550709208\n",
      "Testing accuracy is :------> 0.7026255078551547\n",
      "\n",
      "--------------ROC_AUC_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.8286250207414912\n",
      "Testing accuracy is :------> 0.7883835379141865\n",
      "\n",
      "--------------ACCURACY_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.9592498589960519\n",
      "Testing accuracy is :------> 0.9510574964750117\n"
     ]
    }
   ],
   "source": [
    "# f1_score accuracy\n",
    "print(\"--------------F1_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",f1_score(Y_train,naive_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",f1_score(Y_test,naive_test_pre))\n",
    "print()\n",
    "# matthews_corrcoef\n",
    "print(\"-------------MATTHEWS_CORRCOEF_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",matthews_corrcoef(Y_train,naive_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",matthews_corrcoef(Y_test,naive_test_pre))\n",
    "print()\n",
    "#roc_auc_score\n",
    "print(\"--------------ROC_AUC_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",roc_auc_score(Y_train,naive_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",roc_auc_score(Y_test,naive_test_pre))\n",
    "print()\n",
    "#accuracy score\n",
    "print(\"--------------ACCURACY_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",accuracy_score(Y_train,naive_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",accuracy_score(Y_test,naive_test_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG BOOST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier=xgb.XGBClassifier(random_state=1,learning_rate=0.15,max_depth=5,n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.15,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=300, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=1, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier.fit(training_features,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the training and testing features\n",
    "xgb_train_pre=xgb_classifier.predict(training_features)\n",
    "xgb_test_pre=xgb_classifier.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------F1_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7866539214347489\n",
      "Testing accuracy is :------> 0.7471807930156421\n",
      "\n",
      "-------------MATTHEWS_CORRCOEF_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.7826467348716458\n",
      "Testing accuracy is :------> 0.7392860682741066\n",
      "\n",
      "--------------ROC_AUC_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.8315540971371518\n",
      "Testing accuracy is :------> 0.812217314497102\n",
      "\n",
      "--------------ACCURACY_SCORE------------------------\n",
      "\n",
      "Testing accuracy is :-----> 0.9632841386225481\n",
      "Testing accuracy is :------> 0.9564468118439605\n"
     ]
    }
   ],
   "source": [
    "# f1_score accuracy\n",
    "print(\"--------------F1_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",f1_score(Y_train,xgb_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",f1_score(Y_test,xgb_test_pre))\n",
    "print()\n",
    "# matthews_corrcoef\n",
    "print(\"-------------MATTHEWS_CORRCOEF_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",matthews_corrcoef(Y_train,xgb_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",matthews_corrcoef(Y_test,xgb_test_pre))\n",
    "print()\n",
    "#roc_auc_score\n",
    "print(\"--------------ROC_AUC_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",roc_auc_score(Y_train,xgb_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",roc_auc_score(Y_test,xgb_test_pre))\n",
    "print()\n",
    "#accuracy score\n",
    "print(\"--------------ACCURACY_SCORE------------------------\")\n",
    "print()\n",
    "print(\"Testing accuracy is :----->\",accuracy_score(Y_train,xgb_train_pre))\n",
    "# testing accuracy \n",
    "print(\"Testing accuracy is :------>\",accuracy_score(Y_test,xgb_test_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVING THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the logistic regression model (model : classifier)\n",
    "pickle.dump(Log_classifer,open(\"toxic_logistic.sav\",'wb'))\n",
    "pickle.dump(Ridge_classifier,open(\"toxic_Ridge.sav\",'wb'))\n",
    "pickle.dump(naive_classifer,open(\"toxic_Naive.sav\",'wb'))\n",
    "pickle.dump(xgb_classifier,open(\"toxic_xgb.sav\",'wb'))\n",
    "# saving the vectorizer\n",
    "pickle.dump(word_vctorizer,open(\"vectorizer.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the User comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the user comments\n",
    "def EG_Preprocess(line):\n",
    "    line=re.sub(r\"(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\",\"\",line)\n",
    "    # remove the unnecessary punctuations\n",
    "    line=re.sub(r\"[^,.?a-zA-Z! ]\",\"\",line)\n",
    "    # removing the repeted letters\n",
    "    line=re.sub(r'(\\w)\\1{2,}',r'\\1',line)\n",
    "       \n",
    "    # remove numbers\n",
    "    line=re.sub(r\"[0-9]\",\"\",line)\n",
    "    # Tokienization of the sentence   \n",
    "    tokens_sent=word_tokenize(line)\n",
    "    filt_sent=[]\n",
    "    for word in tokens_sent:\n",
    "        if(word not in stopwords):\n",
    "            filt_sent.append(word)\n",
    "        #stemming\n",
    "        stemmed_sent=[stemmer.stem(x) for x in filt_sent]\n",
    "        # converting the words in to the text\n",
    "    sentt=' '.join(stemmed_sent)\n",
    "    return sentt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing_comments(data):\n",
    "    li=[]\n",
    "    for line in data:\n",
    "        sent=EG_Preprocess(line)\n",
    "        li.append(sent)\n",
    "    \n",
    "\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Machine_Predictions(data):\n",
    "    data=Testing_comments(data)\n",
    "    test_vector=word_vctorizer.transform(data)\n",
    "    l=Log_classifer.predict(test_vector)\n",
    "    r=Ridge_classifier.predict(test_vector)\n",
    "    n=naive_classifer.predict(test_vector)\n",
    "    x=xgb_classifier.predict(test_vector)\n",
    "    dic={\"Logistic\":l,\n",
    "         \"Ridge\":r,\n",
    "        \"Naive\":n,\n",
    "         \"XGB\":x }\n",
    "    return dic\n",
    "def Machine_Predictions_Probs(data):\n",
    "    data=Testing_comments(data)\n",
    "    test_vector=word_vctorizer.transform(data)\n",
    "    l=Log_classifer.predict_proba(test_vector)[:,1]\n",
    "    r=Ridge_classifier.predict(test_vector)\n",
    "    n=naive_classifer.predict_proba(test_vector)[:,1]\n",
    "    x=xgb_classifier.predict_proba(test_vector)[:,1]\n",
    "    dic={\"Logistic\":l,\n",
    "         \"Ridge\":r,\n",
    "        \"Naive\":n,\n",
    "         \"XGB\":x }\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"these fellows are\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Machine_Predictions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_toxic(txt):\n",
    "    toxic=False\n",
    "    d=Machine_Predictions(txt)\n",
    "    \n",
    "    if(d[\"Logistic\"]==1 or d[\"Ridge\"]==1 or d[\"Naive\"]==1 or d[\"XGB\"]==1):\n",
    "        toxic=True\n",
    "    return toxic\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the TWO stages Bag of Words and Machine learing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test=[\"idiot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis(text):\n",
    "    if(Bag_of_Words(text[0]) or mac_toxic(text)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n"
     ]
    }
   ],
   "source": [
    "a=Analysis(user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic': array([0.94670324]),\n",
       " 'Ridge': array([1]),\n",
       " 'Naive': array([0.73620973]),\n",
       " 'XGB': array([0.4761731], dtype=float32)}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Machine_Predictions_Probs(user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mac_toxic(user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic': array([1]),\n",
       " 'Ridge': array([1]),\n",
       " 'Naive': array([1]),\n",
       " 'XGB': array([1])}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Machine_Predictions(user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
